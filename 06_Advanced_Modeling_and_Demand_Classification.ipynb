{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ae175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train a Demand Prediction Model\n",
    "\n",
    "## 6.1 Train a Linear Regression Model\n",
    "\"\"\"\n",
    "\n",
    "# 1. Converting column to a new Series to fully detach from category type\n",
    "df_cleaned['renewable_energy_source'] = df_cleaned['renewable_energy_source'].astype(str).copy()\n",
    "\n",
    "# 2. Ensurign 'Yes' and 'No' are replaced properly\n",
    "df_cleaned['renewable_energy_source'] = df_cleaned['renewable_energy_source'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# 3. Converting to numeric, forcing any unexpected values to NaN\n",
    "df_cleaned['renewable_energy_source'] = pd.to_numeric(df_cleaned['renewable_energy_source'], errors='coerce')\n",
    "\n",
    "# 4. Filling NaN values and converting to int\n",
    "df_cleaned['renewable_energy_source'] = df_cleaned['renewable_energy_source'].fillna(0).astype(int)\n",
    "\n",
    "# Final Check\n",
    "print(df_cleaned['renewable_energy_source'].dtype)  # Should be int64\n",
    "print(df_cleaned['renewable_energy_source'].unique())  # Should be [0, 1]\n",
    "\n",
    "# Ensuring all columns are numeric\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Filingl any NaN values that might have been introduced\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Trainning a Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "\n",
    "print(\"Linear Regression Model Performance:\")\n",
    "print(f\"R² Score: {r2_lr:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lr:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lr:.4f}\")\n",
    "\n",
    "\"\"\"## 6.2 Train an XGBoost Model\"\"\"\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Trainning an XGBoost Model\n",
    "xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\nXGBoost Model Performance:\")\n",
    "print(f\"R² Score: {r2_xgb:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_xgb:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_xgb:.4f}\")\n",
    "\n",
    "\"\"\"## 6.3 Train a Random Forest Model\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Trainning a Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\nRandom Forest Model Performance:\")\n",
    "print(f\"R² Score: {r2_rf:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_rf:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_rf:.4f}\")\n",
    "\n",
    "\"\"\"## 6.4 Hyperparameter Tuning with GridSearchCV\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Performing GridSearchCV\n",
    "grid_search = GridSearchCV(XGBRegressor(random_state=42), param_grid, cv=3, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Getting the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Evaluating best model\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "mae_best = mean_absolute_error(y_test, y_pred_best)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
    "\n",
    "print(\"\\nBest XGBoost Model (After Hyperparameter Tuning):\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"R² Score: {r2_best:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_best:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_best:.4f}\")\n",
    "\n",
    "\"\"\"##  Demand-Based Clustering & Classification\n",
    "\n",
    "## 6.5 Create Demand-Based Clusters Using K-Means\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecting demand-related features\n",
    "demand_features = df_cleaned[[\n",
    "    \"usage_stats_avg_users_per_day\",\n",
    "    \"cost_usd_per_kwh\",\n",
    "    \"distance_to_city_km\",\n",
    "    \"parking_spots\"\n",
    "]]\n",
    "\n",
    "# Normalizing features\n",
    "scaler = StandardScaler()\n",
    "demand_features_scaled = scaler.fit_transform(demand_features)\n",
    "\n",
    "# Fitting K-Means clustering (15 demand clusters)\n",
    "kmeans_demand = KMeans(n_clusters=15, random_state=42, n_init=10)\n",
    "df_cleaned[\"demand_cluster\"] = kmeans_demand.fit_predict(demand_features_scaled)\n",
    "\n",
    "# Check the distribution of demand clusters\n",
    "print(\"Demand Cluster Distribution:\")\n",
    "print(df_cleaned[\"demand_cluster\"].value_counts())\n",
    "\n",
    "\"\"\"## 6.6 Train a Random Forest Classifier\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Defining features for classification\n",
    "selected_features = [\n",
    "    \"cost_usd_per_kwh\",\n",
    "    \"distance_to_city_km\",\n",
    "    \"parking_spots\",\n",
    "    \"charging_capacity_kw\",\n",
    "    \"reviews_rating\"\n",
    "]\n",
    "\n",
    "# Features (X) & Target (y)\n",
    "X_class = df_cleaned[selected_features]\n",
    "y_class = df_cleaned[\"demand_cluster\"]\n",
    "\n",
    "# Train-Test Split (80:20)\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Training Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_class = rf_classifier.predict(X_test_class)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "print(\"\\nRandom Forest Classification Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_class, y_pred_class))\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1. Evaluate RF Classification Model\n",
    "\n",
    "# Calculating R^2 score (since we are classifying demand clusters, we treat them as ordinal values)\n",
    "r2_class = r2_score(y_test_class, y_pred_class)\n",
    "\n",
    "# Calculating Scores\n",
    "mse_class = mean_squared_error(y_test_class, y_pred_class)\n",
    "mae_class = mean_absolute_error(y_test_class, y_pred_class)\n",
    "rmse_class = np.sqrt(mse_class)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRF-Based Demand Classification Model Performance:\")\n",
    "print(f\"R^2 Score: {r2_class:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_class:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_class:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_class:.2f}\")\n",
    "\n",
    "\"\"\"## 6.7 Analyze Demand Clusters\"\"\"\n",
    "\n",
    "# Defining the mapping based on the provided data\n",
    "cluster_to_city = {\n",
    "    0: \"Chicago\",\n",
    "    1: \"Seoul\",\n",
    "    2: \"Cape Town\",\n",
    "    3: \"Berlin\",\n",
    "    4: \"Bangkok\",\n",
    "    5: \"Mexico City\",\n",
    "    6: \"São Paulo\",\n",
    "    7: \"Sydney\",\n",
    "    8: \"Los Angeles\",\n",
    "    9: \"Mumbai\",\n",
    "    10: \"Moscow\",\n",
    "    11: \"Dubai\",\n",
    "    12: \"Beijing\",\n",
    "    13: \"Toronto\",\n",
    "    14: \"San Francisco\"\n",
    "}\n",
    "\n",
    "# Countting stations per demand cluster\n",
    "demand_cluster_counts = df_cleaned[\"demand_cluster\"].value_counts().reset_index()\n",
    "demand_cluster_counts.columns = [\"demand_cluster\", \"station_count\"]\n",
    "demand_cluster_counts[\"city\"] = demand_cluster_counts[\"demand_cluster\"].map(cluster_to_city)\n",
    "\n",
    "# Displaying results\n",
    "demand_cluster_counts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensuring the data is sorted for correct visualization\n",
    "demand_cluster_counts = demand_cluster_counts.sort_values(by=\"station_count\", ascending=False)\n",
    "\n",
    "# Creating formatted labels for the x-axis\n",
    "demand_cluster_counts[\"label\"] = demand_cluster_counts.apply(lambda row: f\"Cluster {row['demand_cluster']} ({row['city']})\", axis=1)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(demand_cluster_counts[\"label\"], demand_cluster_counts[\"station_count\"], color=plt.cm.viridis_r(range(len(demand_cluster_counts))))\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Demand Cluster (City)\")\n",
    "plt.ylabel(\"Number of Charging Stations\")\n",
    "plt.title(\"Demand Cluster Distribution by City\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"## 6.8 Train an XGBoost Classifier\"\"\"\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Trainn9gn an XGBoost Classifier\n",
    "xgb_classifier = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=6, random_state=42)\n",
    "xgb_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_classifier.predict(X_test_class)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy_xgb = accuracy_score(y_test_class, y_pred_xgb)\n",
    "print(\"\\nXGBoost Classification Performance:\")\n",
    "print(f\"Accuracy: {accuracy_xgb:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_class, y_pred_xgb))\n",
    "\n",
    "\"\"\"## 6.9 Tuning XGBoost using GridSearchCV\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definiung hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],  # Testing different values\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Performming Grid SearchCV\n",
    "grid_search = GridSearchCV(XGBClassifier(random_state=42), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Getting the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Trainning the best model\n",
    "best_xgb_model = XGBClassifier(**best_params, random_state=42)\n",
    "best_xgb_model.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Evaluating the tuned model\n",
    "y_pred_best_xgb = best_xgb_model.predict(X_test_class)\n",
    "accuracy_best_xgb = accuracy_score(y_test_class, y_pred_best_xgb)\n",
    "\n",
    "print(f\"\\nTuned XGBoost Accuracy: {accuracy_best_xgb:.4f}\")\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
